{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94cade7-f74d-4d9e-8516-6e17a76aba7c",
   "metadata": {},
   "source": [
    "# Crop Rotation Algorithm Utilizing Minimal Features and Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11fe1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of this notebook is to explore hyperparameter optimization with\n",
    "# Keras, using the crop rotation implementation as the base\n",
    "# https://keras.io/guides/keras_tuner/getting_started/\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Activation, Embedding, Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "745bd5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/2008_2018_CDL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9caf1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_years_from_CDL_data(CDL_data):\n",
    "    \"\"\"\n",
    "    Uses regex and filter to isolate yearly data from CDL dataset\n",
    "    imported with provided SQL query\n",
    "    \n",
    "    Arguments:\n",
    "        CDL_data (pd.DataFrame) : a 2d pandas dataframe with yearly\n",
    "        CDL data; columns must include year e.g. '2010', rows are \n",
    "        field observations\n",
    "        \n",
    "    Returns:\n",
    "        (pd.DataFrame) : a 2d pandas dataframe with only the yearly CDL\n",
    "        crop data\n",
    "    \"\"\"\n",
    "    return CDL_data.filter(regex = '20\\d\\d', axis=1)\n",
    "\n",
    "def remove_ignore_classes_from_yearly_CDL_data(yearly_CDL_data):\n",
    "    \"\"\"\n",
    "    Removes (hard-coded) non-crop classes from yearly CDL dataset\n",
    "    \n",
    "    Arguments:\n",
    "        yearly_CDL_data (pd.DataFrame) : a 2d pandas dataframe with\n",
    "        yearly CDL data, as generated by isolate_years_from_CDL_data;\n",
    "        columns are years e.g. '2010', rows are field observations\n",
    "        \n",
    "    Returns:\n",
    "        cleaned_CDL_data (pd.DataFrame) : a 2d pandas dataframe that\n",
    "        removes observations with ignore_classes    \n",
    "    \"\"\"\n",
    "    ignore_classes = range(81, 196) \n",
    "    \n",
    "    return yearly_CDL_data[~yearly_CDL_data.isin(ignore_classes)].dropna().astype(int)\n",
    "    \n",
    "    \n",
    "def consolidate_nonsupported_crops_to_fallow_from_clean_CDL_data(cleaned_CDL_data):\n",
    "    \"\"\"\n",
    "    Consolidates non-supported crops to fallow class\n",
    "    \n",
    "    Arguments:\n",
    "        cleaned_CDL_data (pd.Dataframe) : a 2d pandas dataframe with cleaned, yearly\n",
    "        CDL data, as generated by remove_ignore_classes_from_yearly_CDL_data; columns\n",
    "        are years e.g. '2010', rows are field observations\n",
    "    \n",
    "    Returns:\n",
    "        consoildated_CDL_data (pd.DataFrame) : a 2d pandas dataframe that\n",
    "        only contains supported crops and fallow classes\n",
    "     \n",
    "    Notes :\n",
    "    \n",
    "    The non-supported crops consolidated into `fallow` (i.e. `61` designation)\n",
    "    crop codes are : \n",
    "    11 : Tobacco, 13 : Popcorn, 14 : Mint, 25 : Other Small Grains, 26 : DblCrop WW/Soy,\n",
    "    27 : Rye, 29 : Millet, 30 : Speltz, 32 : Flaxseed, 33 : Safflower, 35 : Mustard,\n",
    "    37 : Other Hay/Non Alfalfa, 38 : Camelina, 39 : Buckwheat, 42 : Dry Beans, 44 : Other Crops,\n",
    "    46 : Sweet Potatoes, 47 : Misc Vegs and Fruits, 48 : Watermelons, 49 : Onions,\n",
    "    50 : Cucumbers, 51 : Chick Peas, 52 : Lentils, 53 : Peas, 55 : Caneberries, 56 : Hops,\n",
    "    57 : Herbs, 58 : Clover/Wildflowers, 59 : Sod/Grass Seed, 60 : Swithgrass, \n",
    "    61 : Fallow/Idle Cropland, 63 : Forest, 64 : Shrubland, 65 : Barren, 66 Cherries, \n",
    "    67 : Peaches, 68 : Apples, 69 : Grapes, 70 : Christmas Trees, 71 : Other Tree Crops,\n",
    "    72 : Citrus, 74 : Pecans, 75 : Almonds, 76 : Walnuts, 77 : Pears, 81 : Clouds/No Data,\\\n",
    "    82 : Developed, 83 : Water, 87 : Wetlands, 88 : Nonag/Undefined, 92 : Aquaculture,\n",
    "    111 : Open Water, 112 : Perennial Ice/Snow, 121 : Developed/Open Space,\n",
    "    122 : Developed/Low Intensity, 123 : Developed/Med Intensity, 124 : Developed/High Intensity,\n",
    "    131 : Barren, 141 : Deciduous Forest, 142 : Evergreen Forest, 143 : Mixed Forest,\n",
    "    152 : Shrubland, 176 : Grassland/Pasture, 190 : Woody Wetlands, 195 : Herbaceous Wetlands,\n",
    "    204 : Pistacios, 205 : Triticale, 206 : Carrots, 207 : Asparagus, 208 : Garlic,\n",
    "    209 : Canteloups, 210 : Prunes, 211 : Olives, 212 : Oranges, 213 : Honeydew Melons,\n",
    "    214 : Broccoli, 216 : Peppers, 217 : Pomegranates, 218 : Nectarines, 219 : Greens,\n",
    "    220 : Plums, 221 :  Strawberries, 222 : Squash, 223 : Apricots, 224 : Vetch,\n",
    "    225 : Dbl Crop WW/Corn, 226 : Dbl Cropo Oats/Corn, 227 : Lettuce, 229 : Pumpkins,\n",
    "    230 : Dbl Crop Lettuce/Durum Wht, 231 : Dbl Crop Lettuce/Cantaloupe, \n",
    "    232 : Dbl Crop Lettuce/Cotton, 233 : Dbl Crop Lettuce/Barley, 234 : Dbl Crop Durum Wht/Sorghum,\n",
    "    235 : Dbl Crop Barley/Sorghum, 236 : Dbl Crop WinWht/Sorghum, 237 : Dbl Crop Barley/Corn,\n",
    "    238 : Dbl Crop WinWht/Cotton, 239 : Dbl Crop Soybeans/Cotton, 240 : Dbl Crop Soybeans/Oats,\n",
    "    241 : Dbl Crop Corn/Soybeans, 242 : Blueberries, 243 : Cabbage, 244 : Cauliflower, 245 : Celery,\n",
    "    246 : Radishes, 247 : Turnips, 248 : Eggplants, 249 : Gourds, 250 : Cranberries, 254 : Dbl Crop Barley/Soybeans\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    return cleaned_CDL_data.replace([11, 13, 14, 25, 26, 27, 29, 30, 32, 33, 35, 37, 38, 39,\n",
    "                               42, 44, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58,\n",
    "                               59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74,\n",
    "                               75, 76, 77, 81, 82, 83, 87, 88, 92, 111, 112, 121, 122,\n",
    "                               123, 124, 131, 141, 142, 143, 152, 176, 190, 195, 204,\n",
    "                               205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215,\n",
    "                               216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226,\n",
    "                               227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
    "                               239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249,\n",
    "                               250, 254], 61)\n",
    "\n",
    "def create_dummies_for_train_and_testing(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Creates dummy categorical variables for validation and testing from CDL categories\n",
    "    \n",
    "    Arguments :\n",
    "        train_data (pd.series) : a 1d pandas series of integer encoded\n",
    "        categorical labels (i.e. y) that will be used for training\n",
    "        test_data (pd.series) : a 1d pandas series of integer encoded categorical \n",
    "        lables (i.e. y) that will be use for testing\n",
    "        \n",
    "    Returns :\n",
    "        train_y (pd.Series) : a 2d pandas dataframe of one-hot encoded\n",
    "        categorical lables for training\n",
    "        test_y (pd.Series) : a 2d pandas dataframe of one-hot encoded\n",
    "        categorical lables for testing\n",
    "        \n",
    "    Notes\n",
    "    \n",
    "    Pool both target sets (i.e. training and testing targets) into one set in order to get all possible dummies given\n",
    "    the data, We should probably add an additional category at somepoint to handle unknown classifications\n",
    "    \"\"\"\n",
    "    integer_encoded_labels = pd.concat([train_data, test_data])\n",
    "    dummy_lables = pd.get_dummies(integer_encoded_labels)\n",
    "    train_y = dummy_lables.iloc[:train_data.shape[0],:]\n",
    "    test_y = dummy_lables.iloc[train_data.shape[0]:,:]\n",
    "    \n",
    "    return train_y, test_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff4e67db",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_CDL_data = isolate_years_from_CDL_data(df)\n",
    "cleaned_CDL_data = remove_ignore_classes_from_yearly_CDL_data(yearly_CDL_data)\n",
    "consolidated_CDL_data = consolidate_nonsupported_crops_to_fallow_from_clean_CDL_data(cleaned_CDL_data)\n",
    "\n",
    "\"\"\"\n",
    "Split data into train and test sets, in this case, 2008-2017 for the train set (i.e. using 2008-2016 data to \n",
    "predict 2017) and 2009-2018 for the test set (i.e. i.e. using 2009-2017 data to predict 2018). While this \n",
    "makes a suboptimal divide for training/testing of 50/50, it was empirically determined to generate the highest\n",
    "accuracy. As future years are added, this should be rechecked\n",
    "\"\"\"\n",
    "train_y, test_y = create_dummies_for_train_and_testing(consolidated_CDL_data['2017'],\n",
    "                                                         consolidated_CDL_data['2018'])\n",
    "\n",
    "train_x = consolidated_CDL_data.drop('2018', axis = 1)\n",
    "test_x = consolidated_CDL_data.drop('2008', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3776a8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 9, 10)             620       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5, 50)             2550      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 5, 50)             0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               10200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 21)                4221      \n",
      "=================================================================\n",
      "Total params: 17,591\n",
      "Trainable params: 17,591\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set parameters of the Deep Convolutional Temporal Network (DCTN)\n",
    "\n",
    "max_features = 62 # max class/category value; used for defining spatial embeddings\n",
    "embedding_dims = 10 # number of divisions within classes for converting from categorical to continuous classes\n",
    "maxlen = 9 # maximum length of the input, in this case number of years in the range\n",
    "filters = 50 # empirically determined for highest accuracy/computation time trade-off\n",
    "kernel_size = 5 # maximum length for facilitating two 1D Convolusional Layers\n",
    "hidden_dims = 200 # empirically determined for highest accuracy/computation time trade-off\n",
    "batch_size = 200 # empirically determined for optimal performance given current set\n",
    "epochs = 1 # learning converges earlier, but 10 provides a safe cushion for this set\n",
    "\n",
    "# Define DCTN architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length = maxlen))\n",
    "\n",
    "model.add(Conv1D(filters, kernel_size, padding = 'valid', activation = 'relu', \n",
    "                 strides = 1, kernel_initializer = 'glorot_normal'))\n",
    "model.add(Dropout(rate = 0.1))\n",
    "\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(hidden_dims, activation = 'relu', kernel_initializer = 'glorot_normal'))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "\n",
    "model.add(Dense(train_y.shape[1], activation ='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f937fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/crop_rotation_prediction/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1955996 samples, validate on 488999 samples\n",
      "Epoch 1/1\n",
      "1955996/1955996 [==============================] - 148s 76us/step - loss: 0.9401 - accuracy: 0.6675 - val_loss: 0.7596 - val_accuracy: 0.7295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f99600bedd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainx.drop('2017', axis = 1), train_y, batch_size = batch_size, epochs = 1, validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "387210de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Embedding(62, 10, input_length = 9))\n",
    "    model.add(keras.layers.Conv1D(50, 5, padding = 'valid', activation = 'relu', \n",
    "                     strides = hp.Choice('units', [1,2,3]), kernel_initializer = 'glorot_normal'))\n",
    "    model.add(keras.layers.Dropout(rate = 0.1))\n",
    "\n",
    "    model.add(keras.layers.GlobalMaxPooling1D())\n",
    "\n",
    "    model.add(keras.layers.Dense(200, activation = 'relu', kernel_initializer = 'glorot_normal'))\n",
    "    model.add(keras.layers.Dropout(rate = 0.2))\n",
    "\n",
    "    model.add(keras.layers.Dense(train_y.shape[1], activation ='softmax'))\n",
    "    #model.summary()\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#sparse_categorical_crossentropy\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e37a3573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner = kt.RandomSearch(model_builder, objective = 'val_loss', max_trials = 5, directory='../data/')\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    hypermodel=model_builder,\n",
    "    # No objective to specify.\n",
    "    # Objective is the return value of `HyperModel.fit()`.\n",
    "    max_trials=3,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"custom_eval\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(train_x.drop('2017', axis = 1).loc[:1000], train_y.loc[:1000], epochs=1, validation_split = 0.2)\n",
    "best_model = tuner.get_best_models()[0]\n",
    "\n",
    "#NOT SUPPORTED IN TF 2.0, please create and compile the model under distribution strategy scope instead of passing it to compile\n",
    "\n",
    "#https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97b08102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>386 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      2008  2009  2010  2011  2012  2013  2014  2015  2016\n",
       "1       61    61     1     5     1     5     5     5     1\n",
       "2       61    61     1     5     1     5     5     5     5\n",
       "3        5     5    61     5     5     5     5     5     5\n",
       "4        5     5     5     1     1     1     5     5     5\n",
       "5        5     5     5     5     1     5    61     5     5\n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...\n",
       "995     61    61    61    61    61    61    61    61    61\n",
       "996     61    61    61    61    61    61    61    61    61\n",
       "997     61    61    61    61    61    61    61    61    61\n",
       "998     61    61    61    61    61    61    61    61    61\n",
       "1000    61    61    61    61    61    61    61    61    61\n",
       "\n",
       "[386 rows x 9 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.drop('2017', axis = 1).loc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aea333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FCRBM",
   "language": "python",
   "name": "fcrbm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
