{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11fe1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of this notebook is to explore hyperparameter optimization with\n",
    "# Keras, using the crop rotation implementation as the base\n",
    "# https://keras.io/guides/keras_tuner/getting_started/\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Activation, Embedding, Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "import keras_tuner as kt\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "745bd5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/2008_2018_CDL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9caf1fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_years_from_CDL_data(CDL_data):\n",
    "    \"\"\"\n",
    "    Uses regex and filter to isolate yearly data from CDL dataset\n",
    "    imported with provided SQL query\n",
    "    \n",
    "    Arguments:\n",
    "        CDL_data (pd.DataFrame) : a 2d pandas dataframe with yearly\n",
    "        CDL data; columns must include year e.g. '2010', rows are \n",
    "        field observations\n",
    "        \n",
    "    Returns:\n",
    "        (pd.DataFrame) : a 2d pandas dataframe with only the yearly CDL\n",
    "        crop data\n",
    "    \"\"\"\n",
    "    return CDL_data.filter(regex = '20\\d\\d', axis=1)\n",
    "\n",
    "def remove_ignore_classes_from_yearly_CDL_data(yearly_CDL_data):\n",
    "    \"\"\"\n",
    "    Removes (hard-coded) non-crop classes from yearly CDL dataset\n",
    "    \n",
    "    Arguments:\n",
    "        yearly_CDL_data (pd.DataFrame) : a 2d pandas dataframe with\n",
    "        yearly CDL data, as generated by isolate_years_from_CDL_data;\n",
    "        columns are years e.g. '2010', rows are field observations\n",
    "        \n",
    "    Returns:\n",
    "        cleaned_CDL_data (pd.DataFrame) : a 2d pandas dataframe that\n",
    "        removes observations with ignore_classes    \n",
    "    \"\"\"\n",
    "    ignore_classes = range(81, 196) \n",
    "    \n",
    "    return yearly_CDL_data[~yearly_CDL_data.isin(ignore_classes)].dropna().astype(int)\n",
    "    \n",
    "    \n",
    "def consolidate_nonsupported_crops_to_fallow_from_clean_CDL_data(cleaned_CDL_data):\n",
    "    \"\"\"\n",
    "    Consolidates non-supported crops to fallow class\n",
    "    \n",
    "    Arguments:\n",
    "        cleaned_CDL_data (pd.Dataframe) : a 2d pandas dataframe with cleaned, yearly\n",
    "        CDL data, as generated by remove_ignore_classes_from_yearly_CDL_data; columns\n",
    "        are years e.g. '2010', rows are field observations\n",
    "    \n",
    "    Returns:\n",
    "        consoildated_CDL_data (pd.DataFrame) : a 2d pandas dataframe that\n",
    "        only contains supported crops and fallow classes\n",
    "     \n",
    "    Notes :\n",
    "    \n",
    "    The non-supported crops consolidated into `fallow` (i.e. `61` designation)\n",
    "    crop codes are : \n",
    "    11 : Tobacco, 13 : Popcorn, 14 : Mint, 25 : Other Small Grains, 26 : DblCrop WW/Soy,\n",
    "    27 : Rye, 29 : Millet, 30 : Speltz, 32 : Flaxseed, 33 : Safflower, 35 : Mustard,\n",
    "    37 : Other Hay/Non Alfalfa, 38 : Camelina, 39 : Buckwheat, 42 : Dry Beans, 44 : Other Crops,\n",
    "    46 : Sweet Potatoes, 47 : Misc Vegs and Fruits, 48 : Watermelons, 49 : Onions,\n",
    "    50 : Cucumbers, 51 : Chick Peas, 52 : Lentils, 53 : Peas, 55 : Caneberries, 56 : Hops,\n",
    "    57 : Herbs, 58 : Clover/Wildflowers, 59 : Sod/Grass Seed, 60 : Swithgrass, \n",
    "    61 : Fallow/Idle Cropland, 63 : Forest, 64 : Shrubland, 65 : Barren, 66 Cherries, \n",
    "    67 : Peaches, 68 : Apples, 69 : Grapes, 70 : Christmas Trees, 71 : Other Tree Crops,\n",
    "    72 : Citrus, 74 : Pecans, 75 : Almonds, 76 : Walnuts, 77 : Pears, 81 : Clouds/No Data,\\\n",
    "    82 : Developed, 83 : Water, 87 : Wetlands, 88 : Nonag/Undefined, 92 : Aquaculture,\n",
    "    111 : Open Water, 112 : Perennial Ice/Snow, 121 : Developed/Open Space,\n",
    "    122 : Developed/Low Intensity, 123 : Developed/Med Intensity, 124 : Developed/High Intensity,\n",
    "    131 : Barren, 141 : Deciduous Forest, 142 : Evergreen Forest, 143 : Mixed Forest,\n",
    "    152 : Shrubland, 176 : Grassland/Pasture, 190 : Woody Wetlands, 195 : Herbaceous Wetlands,\n",
    "    204 : Pistacios, 205 : Triticale, 206 : Carrots, 207 : Asparagus, 208 : Garlic,\n",
    "    209 : Canteloups, 210 : Prunes, 211 : Olives, 212 : Oranges, 213 : Honeydew Melons,\n",
    "    214 : Broccoli, 216 : Peppers, 217 : Pomegranates, 218 : Nectarines, 219 : Greens,\n",
    "    220 : Plums, 221 :  Strawberries, 222 : Squash, 223 : Apricots, 224 : Vetch,\n",
    "    225 : Dbl Crop WW/Corn, 226 : Dbl Cropo Oats/Corn, 227 : Lettuce, 229 : Pumpkins,\n",
    "    230 : Dbl Crop Lettuce/Durum Wht, 231 : Dbl Crop Lettuce/Cantaloupe, \n",
    "    232 : Dbl Crop Lettuce/Cotton, 233 : Dbl Crop Lettuce/Barley, 234 : Dbl Crop Durum Wht/Sorghum,\n",
    "    235 : Dbl Crop Barley/Sorghum, 236 : Dbl Crop WinWht/Sorghum, 237 : Dbl Crop Barley/Corn,\n",
    "    238 : Dbl Crop WinWht/Cotton, 239 : Dbl Crop Soybeans/Cotton, 240 : Dbl Crop Soybeans/Oats,\n",
    "    241 : Dbl Crop Corn/Soybeans, 242 : Blueberries, 243 : Cabbage, 244 : Cauliflower, 245 : Celery,\n",
    "    246 : Radishes, 247 : Turnips, 248 : Eggplants, 249 : Gourds, 250 : Cranberries, 254 : Dbl Crop Barley/Soybeans\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    return cleaned_CDL_data.replace([11, 13, 14, 25, 26, 27, 29, 30, 32, 33, 35, 37, 38, 39,\n",
    "                               42, 44, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58,\n",
    "                               59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74,\n",
    "                               75, 76, 77, 81, 82, 83, 87, 88, 92, 111, 112, 121, 122,\n",
    "                               123, 124, 131, 141, 142, 143, 152, 176, 190, 195, 204,\n",
    "                               205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215,\n",
    "                               216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226,\n",
    "                               227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
    "                               239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249,\n",
    "                               250, 254], 61)\n",
    "\n",
    "def create_dummies_for_train_and_testing(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Creates dummy categorical variables for validation and testing from CDL categories\n",
    "    \n",
    "    Arguments :\n",
    "        train_data (pd.series) : a 1d pandas series of integer encoded\n",
    "        categorical labels (i.e. y) that will be used for training\n",
    "        test_data (pd.series) : a 1d pandas series of integer encoded categorical \n",
    "        lables (i.e. y) that will be use for testing\n",
    "        \n",
    "    Returns :\n",
    "        train_y (pd.Series) : a 2d pandas dataframe of one-hot encoded\n",
    "        categorical lables for training\n",
    "        test_y (pd.Series) : a 2d pandas dataframe of one-hot encoded\n",
    "        categorical lables for testing\n",
    "        \n",
    "    Notes\n",
    "    \n",
    "    Pool both target sets (i.e. training and testing targets) into one set in order to get all possible dummies given\n",
    "    the data, We should probably add an additional category at somepoint to handle unknown classifications\n",
    "    \"\"\"\n",
    "    integer_encoded_labels = pd.concat([train_data, test_data])\n",
    "    dummy_lables = pd.get_dummies(integer_encoded_labels)\n",
    "    train_y = dummy_lables.iloc[:train_data.shape[0],:]\n",
    "    test_y = dummy_lables.iloc[train_data.shape[0]:,:]\n",
    "    \n",
    "    return train_y, test_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff4e67db",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_CDL_data = isolate_years_from_CDL_data(df)\n",
    "cleaned_CDL_data = remove_ignore_classes_from_yearly_CDL_data(yearly_CDL_data)\n",
    "consolidated_CDL_data = consolidate_nonsupported_crops_to_fallow_from_clean_CDL_data(cleaned_CDL_data)\n",
    "\n",
    "\"\"\"\n",
    "Split data into train and test sets, in this case, 2008-2017 for the train set (i.e. using 2008-2016 data to \n",
    "predict 2017) and 2009-2018 for the test set (i.e. i.e. using 2009-2017 data to predict 2018). While this \n",
    "makes a suboptimal divide for training/testing of 50/50, it was empirically determined to generate the highest\n",
    "accuracy. As future years are added, this should be rechecked\n",
    "\"\"\"\n",
    "train_y, test_y = create_dummies_for_train_and_testing(consolidated_CDL_data['2017'],\n",
    "                                                         consolidated_CDL_data['2018'])\n",
    "\n",
    "train_x = consolidated_CDL_data.drop('2018', axis = 1)\n",
    "test_x = consolidated_CDL_data.drop('2008', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3776a8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 9, 10)             620       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 5, 50)             2550      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 5, 50)             0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               10200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 21)                4221      \n",
      "=================================================================\n",
      "Total params: 17,591\n",
      "Trainable params: 17,591\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set parameters of the Deep Convolutional Temporal Network (DCTN)\n",
    "\n",
    "max_features = 62 # max class/category value; used for defining spatial embeddings\n",
    "embedding_dims = 10 # number of divisions within classes for converting from categorical to continuous classes\n",
    "maxlen = 9 # maximum length of the input, in this case number of years in the range\n",
    "filters = 50 # empirically determined for highest accuracy/computation time trade-off\n",
    "kernel_size = 5 # maximum length for facilitating two 1D Convolusional Layers\n",
    "hidden_dims = 200 # empirically determined for highest accuracy/computation time trade-off\n",
    "batch_size = 200 # empirically determined for optimal performance given current set\n",
    "epochs = 1 # learning converges earlier, but 10 provides a safe cushion for this set\n",
    "\n",
    "# Define DCTN architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length = maxlen))\n",
    "\n",
    "model.add(Conv1D(filters, kernel_size, padding = 'valid', activation = 'relu', \n",
    "                 strides = 1, kernel_initializer = 'glorot_normal'))\n",
    "model.add(Dropout(rate = 0.1))\n",
    "\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(hidden_dims, activation = 'relu', kernel_initializer = 'glorot_normal'))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "\n",
    "model.add(Dense(train_y.shape[1], activation ='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f937fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/crop_rotation_prediction/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1955996 samples, validate on 488999 samples\n",
      "Epoch 1/1\n",
      "1955996/1955996 [==============================] - 148s 76us/step - loss: 0.9401 - accuracy: 0.6675 - val_loss: 0.7596 - val_accuracy: 0.7295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f99600bedd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainx.drop('2017', axis = 1), train_y, batch_size = batch_size, epochs = 1, validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "387210de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Embedding(62, 10, input_length = 9))\n",
    "    model.add(keras.layers.Conv1D(50, 5, padding = 'valid', activation = 'relu', \n",
    "                     strides = hp.Choice('units', [1,2,3]), kernel_initializer = 'glorot_normal'))\n",
    "    model.add(keras.layers.Dropout(rate = 0.1))\n",
    "\n",
    "    model.add(keras.layers.GlobalMaxPooling1D())\n",
    "\n",
    "    model.add(keras.layers.Dense(200, activation = 'relu', kernel_initializer = 'glorot_normal'))\n",
    "    model.add(keras.layers.Dropout(rate = 0.2))\n",
    "\n",
    "    model.add(keras.layers.Dense(train_y.shape[1], activation ='softmax'))\n",
    "    #model.summary()\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#sparse_categorical_crossentropy\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e37a3573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner = kt.RandomSearch(model_builder, objective = 'val_loss', max_trials = 5, directory='../data/')\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    hypermodel=model_builder,\n",
    "    # No objective to specify.\n",
    "    # Objective is the return value of `HyperModel.fit()`.\n",
    "    max_trials=3,\n",
    "    overwrite=True,\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"custom_eval\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e1d6729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "1                 |?                 |units\n",
      "\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 308 samples, validate on 78 samples\n",
      "256/308 [=======================>......] - ETA: 0s - loss: 3.0035 - accuracy: 0.4727    "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'distribute_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-30ce3d63e36d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2017'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crop_rotation_prediction/lib/python3.7/site-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0;31m# `results` is None indicates user updated oracle in `run_trial()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crop_rotation_prediction/lib/python3.7/site-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mobj_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crop_rotation_prediction/lib/python3.7/site-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         tuner_utils.validate_trial_results(\n\u001b[1;32m    224\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"HyperModel.fit()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crop_rotation_prediction/lib/python3.7/site-packages/keras_tuner/engine/hypermodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \"\"\"\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crop_rotation_prediction/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/anaconda3/envs/crop_rotation_prediction/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/anaconda3/envs/crop_rotation_prediction/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m       \u001b[0;31m# Epochs only apply to `fit`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m     \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crop_rotation_prediction/lib/python3.7/site-packages/tensorflow_core/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crop_rotation_prediction/lib/python3.7/site-packages/keras_tuner/engine/tuner_utils.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;31m# objective is specified, or objective is computed and returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;31m# after `fit()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mcurrent_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/crop_rotation_prediction/lib/python3.7/site-packages/keras_tuner/engine/tuner_utils.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Create temporary saved model files on non-chief workers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         write_filepath = ds_utils.write_filepath(\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         )\n\u001b[1;32m    222\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'distribute_strategy'"
     ]
    }
   ],
   "source": [
    "tuner.search(train_x.drop('2017', axis = 1).loc[:1000], train_y.loc[:1000], epochs=1, validation_split = 0.2)\n",
    "best_model = tuner.get_best_models()[0]\n",
    "\n",
    "#NOT SUPPORTED IN TF 2.0, please create and compile the model under distribution strategy scope instead of passing it to compile\n",
    "\n",
    "#https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97b08102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>386 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      2008  2009  2010  2011  2012  2013  2014  2015  2016\n",
       "1       61    61     1     5     1     5     5     5     1\n",
       "2       61    61     1     5     1     5     5     5     5\n",
       "3        5     5    61     5     5     5     5     5     5\n",
       "4        5     5     5     1     1     1     5     5     5\n",
       "5        5     5     5     5     1     5    61     5     5\n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...\n",
       "995     61    61    61    61    61    61    61    61    61\n",
       "996     61    61    61    61    61    61    61    61    61\n",
       "997     61    61    61    61    61    61    61    61    61\n",
       "998     61    61    61    61    61    61    61    61    61\n",
       "1000    61    61    61    61    61    61    61    61    61\n",
       "\n",
       "[386 rows x 9 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.drop('2017', axis = 1).loc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aea333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crop_rotation_prediction",
   "language": "python",
   "name": "crop_rotation_prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
