{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# The purpose of this notebook is to articulate the path to training a crop\n",
    "# rotation model, manipulating data and creating a keras workflow.\n",
    "# Tested in the subsequent \"Test\" notebook\n",
    "#\n",
    "# Running all cells takes approximately 10 mins with the bulk of time spent on\n",
    "# training and evaluation of the CDL dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Conv1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CDL dataset. This can be done through a database connection as well\n",
    "\n",
    "df = pd.read_csv('../data/2008_2018_CDL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_years_from_CDL_data(CDL_data):\n",
    "    \"\"\"\n",
    "    Uses regex and filter to isolate yearly data from CDL dataset\n",
    "    imported with provided SQL query\n",
    "    \n",
    "    Arguments:\n",
    "        CDL_data (pd.DataFrame) : a 2d pandas dataframe with yearly\n",
    "        CDL data; columns must include year e.g. '2010', rows are \n",
    "        field observations\n",
    "        \n",
    "    Returns:\n",
    "        (pd.DataFrame) : a 2d pandas dataframe with only the yearly CDL\n",
    "        crop data\n",
    "    \"\"\"\n",
    "    return CDL_data.filter(regex = '20\\d\\d', axis=1)\n",
    "\n",
    "def remove_ignore_classes_from_yearly_CDL_data(yearly_CDL_data):\n",
    "    \"\"\"\n",
    "    Removes (hard-coded) non-crop classes from yearly CDL dataset\n",
    "    \n",
    "    Arguments:\n",
    "        yearly_CDL_data (pd.DataFrame) : a 2d pandas dataframe with\n",
    "        yearly CDL data, as generated by isolate_years_from_CDL_data;\n",
    "        columns are years e.g. '2010', rows are field observations\n",
    "        \n",
    "    Returns:\n",
    "        cleaned_CDL_data (pd.DataFrame) : a 2d pandas dataframe that\n",
    "        removes observations with ignore_classes    \n",
    "    \"\"\"\n",
    "    ignore_classes = range(81, 196) \n",
    "    \n",
    "    return yearly_CDL_data[~yearly_CDL_data.isin(ignore_classes)].dropna().astype(int)\n",
    "    \n",
    "    \n",
    "def consolidate_nonsupported_crops_to_fallow_from_clean_CDL_data(cleaned_CDL_data):\n",
    "    \"\"\"\n",
    "    Consolidates non-supported crops to fallow class\n",
    "    \n",
    "    Arguments:\n",
    "        cleaned_CDL_data (pd.Dataframe) : a 2d pandas dataframe with cleaned, yearly\n",
    "        CDL data, as generated by remove_ignore_classes_from_yearly_CDL_data; columns\n",
    "        are years e.g. '2010', rows are field observations\n",
    "    \n",
    "    Returns:\n",
    "        consoildated_CDL_data (pd.DataFrame) : a 2d pandas dataframe that\n",
    "        only contains supported crops and fallow classes\n",
    "     \n",
    "    Notes :\n",
    "    \n",
    "    The non-supported crops consolidated into `fallow` (i.e. `61` designation)\n",
    "    crop codes are : \n",
    "    11 : Tobacco, 13 : Popcorn, 14 : Mint, 25 : Other Small Grains, 26 : DblCrop WW/Soy,\n",
    "    27 : Rye, 29 : Millet, 30 : Speltz, 32 : Flaxseed, 33 : Safflower, 35 : Mustard,\n",
    "    37 : Other Hay/Non Alfalfa, 38 : Camelina, 39 : Buckwheat, 42 : Dry Beans, 44 : Other Crops,\n",
    "    46 : Sweet Potatoes, 47 : Misc Vegs and Fruits, 48 : Watermelons, 49 : Onions,\n",
    "    50 : Cucumbers, 51 : Chick Peas, 52 : Lentils, 53 : Peas, 55 : Caneberries, 56 : Hops,\n",
    "    57 : Herbs, 58 : Clover/Wildflowers, 59 : Sod/Grass Seed, 60 : Swithgrass, \n",
    "    61 : Fallow/Idle Cropland, 63 : Forest, 64 : Shrubland, 65 : Barren, 66 Cherries, \n",
    "    67 : Peaches, 68 : Apples, 69 : Grapes, 70 : Christmas Trees, 71 : Other Tree Crops,\n",
    "    72 : Citrus, 74 : Pecans, 75 : Almonds, 76 : Walnuts, 77 : Pears, 81 : Clouds/No Data,\\\n",
    "    82 : Developed, 83 : Water, 87 : Wetlands, 88 : Nonag/Undefined, 92 : Aquaculture,\n",
    "    111 : Open Water, 112 : Perennial Ice/Snow, 121 : Developed/Open Space,\n",
    "    122 : Developed/Low Intensity, 123 : Developed/Med Intensity, 124 : Developed/High Intensity,\n",
    "    131 : Barren, 141 : Deciduous Forest, 142 : Evergreen Forest, 143 : Mixed Forest,\n",
    "    152 : Shrubland, 176 : Grassland/Pasture, 190 : Woody Wetlands, 195 : Herbaceous Wetlands,\n",
    "    204 : Pistacios, 205 : Triticale, 206 : Carrots, 207 : Asparagus, 208 : Garlic,\n",
    "    209 : Canteloups, 210 : Prunes, 211 : Olives, 212 : Oranges, 213 : Honeydew Melons,\n",
    "    214 : Broccoli, 216 : Peppers, 217 : Pomegranates, 218 : Nectarines, 219 : Greens,\n",
    "    220 : Plums, 221 :  Strawberries, 222 : Squash, 223 : Apricots, 224 : Vetch,\n",
    "    225 : Dbl Crop WW/Corn, 226 : Dbl Cropo Oats/Corn, 227 : Lettuce, 229 : Pumpkins,\n",
    "    230 : Dbl Crop Lettuce/Durum Wht, 231 : Dbl Crop Lettuce/Cantaloupe, \n",
    "    232 : Dbl Crop Lettuce/Cotton, 233 : Dbl Crop Lettuce/Barley, 234 : Dbl Crop Durum Wht/Sorghum,\n",
    "    235 : Dbl Crop Barley/Sorghum, 236 : Dbl Crop WinWht/Sorghum, 237 : Dbl Crop Barley/Corn,\n",
    "    238 : Dbl Crop WinWht/Cotton, 239 : Dbl Crop Soybeans/Cotton, 240 : Dbl Crop Soybeans/Oats,\n",
    "    241 : Dbl Crop Corn/Soybeans, 242 : Blueberries, 243 : Cabbage, 244 : Cauliflower, 245 : Celery,\n",
    "    246 : Radishes, 247 : Turnips, 248 : Eggplants, 249 : Gourds, 250 : Cranberries, 254 : Dbl Crop Barley/Soybeans\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    return cleaned_CDL_data.replace([11, 13, 14, 25, 26, 27, 29, 30, 32, 33, 35, 37, 38, 39,\n",
    "                               42, 44, 46, 47, 48, 49, 50, 51, 52, 53, 55, 56, 57, 58,\n",
    "                               59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 74,\n",
    "                               75, 76, 77, 81, 82, 83, 87, 88, 92, 111, 112, 121, 122,\n",
    "                               123, 124, 131, 141, 142, 143, 152, 176, 190, 195, 204,\n",
    "                               205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215,\n",
    "                               216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226,\n",
    "                               227, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
    "                               239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249,\n",
    "                               250, 254], 61)\n",
    "\n",
    "def create_dummies_for_train_and_testing(train_data, test_data):\n",
    "    \"\"\"\n",
    "    Creates dummy categorical variables for validation and testing from CDL categories\n",
    "    \n",
    "    Arguments :\n",
    "        train_data (pd.series) : a 1d pandas series of integer encoded\n",
    "        categorical labels (i.e. y) that will be used for training\n",
    "        test_data (pd.series) : a 1d pandas series of integer encoded categorical \n",
    "        lables (i.e. y) that will be use for testing\n",
    "        \n",
    "    Returns :\n",
    "        train_y (pd.Series) : a 2d pandas dataframe of one-hot encoded\n",
    "        categorical lables for training\n",
    "        test_y (pd.Series) : a 2d pandas dataframe of one-hot encoded\n",
    "        categorical lables for testing\n",
    "        \n",
    "    Notes\n",
    "    \n",
    "    Pool both target sets (i.e. training and testing targets) into one set in order to get all possible dummies given\n",
    "    the data, We should probably add an additional category at somepoint to handle unknown classifications\n",
    "    \"\"\"\n",
    "    integer_encoded_labels = pd.concat([train_data, test_data])\n",
    "    dummy_lables = pd.get_dummies(integer_encoded_labels)\n",
    "    train_y = dummy_lables.iloc[:train_data.shape[0],:]\n",
    "    test_y = dummy_lables.iloc[train_data.shape[0]:,:]\n",
    "    \n",
    "    return train_y, test_y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_CDL_data = isolate_years_from_CDL_data(df)\n",
    "cleaned_CDL_data = remove_ignore_classes_from_yearly_CDL_data(yearly_CDL_data)\n",
    "consolidated_CDL_data = consolidate_nonsupported_crops_to_fallow_from_clean_CDL_data(cleaned_CDL_data)\n",
    "\n",
    "\"\"\"\n",
    "Split data into train and test sets, in this case, 2008-2017 for the train set (i.e. using 2008-2016 data to \n",
    "predict 2017) and 2009-2018 for the test set (i.e. i.e. using 2009-2017 data to predict 2018). While this \n",
    "makes a suboptimal divide for training/testing of 50/50, it was empirically determined to generate the highest\n",
    "accuracy. As future years are added, this should be rechecked\n",
    "\"\"\"\n",
    "train_y, test_y = create_dummies_for_train_and_testing(consolidated_CDL_data['2017'],\n",
    "                                                         consolidated_CDL_data['2018'])\n",
    "\n",
    "train_x = consolidated_CDL_data.drop('2018', axis = 1)\n",
    "test_x = consolidated_CDL_data.drop('2008', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 9, 10)             620       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 5, 50)             2550      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 50)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 50)             12550     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1, 50)             0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               10200     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 21)                4221      \n",
      "=================================================================\n",
      "Total params: 150,741\n",
      "Trainable params: 150,741\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Set parameters of the Deep Convolutional Temporal Network (DCTN)\n",
    "\n",
    "max_features = 62 # max class/category value; used for defining spatial embeddings\n",
    "embedding_dims = 10 # number of divisions within classes for converting from categorical to continuous classes\n",
    "maxlen = 9 # maximum length of the input, in this case number of years in the range\n",
    "filters = 50 # empirically determined for highest accuracy/computation time trade-off\n",
    "kernel_size = 5 # maximum length for facilitating two 1D Convolusional Layers\n",
    "hidden_dims = 200 # empirically determined for highest accuracy/computation time trade-off\n",
    "batch_size = 200 # empirically determined for optimal performance given current set\n",
    "epochs = 10 # learning converges earlier, but 10 provides a safe cushion for this set\n",
    "\n",
    "# Define DCTN architecture\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_dims, input_length = maxlen))\n",
    "\n",
    "model.add(Conv1D(filters, kernel_size, padding = 'valid', activation = 'relu', \n",
    "                 strides = 1, kernel_initializer = 'glorot_normal'))\n",
    "model.add(Dropout(rate = 0.1))\n",
    "model.add(Conv1D(filters, kernel_size, padding ='valid', activation ='relu', \n",
    "                 strides = 1, kernel_initializer = 'glorot_normal'))\n",
    "model.add(Dropout(rate = 0.1))\n",
    "\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(hidden_dims, activation = 'relu', kernel_initializer = 'glorot_normal'))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "model.add(Dense(hidden_dims, activation = 'relu', kernel_initializer = 'glorot_normal'))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "model.add(Dense(hidden_dims, activation = 'relu', kernel_initializer = 'glorot_normal'))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "model.add(Dense(hidden_dims, activation = 'relu', kernel_initializer = 'glorot_normal'))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "\n",
    "model.add(Dense(train_y.shape[1], activation ='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/crop_rotation_prediction/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1955996 samples, validate on 488999 samples\n",
      "Epoch 1/1\n",
      "1955996/1955996 [==============================] - 307s 157us/step - loss: 0.9334 - accuracy: 0.6864 - val_loss: 0.7390 - val_accuracy: 0.7435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f85b081c190>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting model to training data. Warning flags can be ignored for now and should be resolved when Keras\n",
    "# versions update\n",
    "\n",
    "#HARD ADJUSTING EPOCHS JUST TO DEMO EXECUTION; WARNING: CRANKS HOT\n",
    "\n",
    "model.fit(train_x.drop('2017', axis = 1), train_y, batch_size = batch_size, epochs = 1, validation_split = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2444995/2444995 [==============================] - 288s 118us/step\n",
      "\n",
      "Test score: 0.8760982792677529\n",
      "Test accuracy: 0.717538058757782\n"
     ]
    }
   ],
   "source": [
    "# Testing the model and reporting scores\n",
    "\n",
    "score = model.evaluate(test_x.drop('2018', axis = 1), testy, verbose = 1)\n",
    "\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicitng single instance. Will have to check and replace `fallow` crops.\n",
    "\n",
    "smallTest = test_x.iloc[0:3,:-1]\n",
    "classLabels = dummys.columns\n",
    "_prob = model.predict(smallTest)\n",
    "_classes = _prob.argmax(axis = -1)\n",
    "classes = classLabels[_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([1, 5, 5], dtype='int64')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predicitng test set. `Fallow` crops already replaced above.\n",
    "\n",
    "# _prob = model.predict(testx.drop('2018', axis = 1))\n",
    "# _classes = _prob.argmax(axis = -1)\n",
    "# classes = classLabels[_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving model\n",
    "\n",
    "# # RENAME TO DESIRED DESCRIPTORS\n",
    "\n",
    "# model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FCRBM",
   "language": "python",
   "name": "fcrbm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
